apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: kube-system
data:
  rules.yml: |
    groups:
    - name: k8s.rules
      rules:
      #noderulesANDalertmanagerConfig
      - record: instance:node_cpu:rate:sum
        expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!~"^(?:guest.*)$"}[3m])) BY (nodename)
      - record: instance:node_filesystem_usage:sum
        expr: sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"})) BY (nodename)
      - record: instance:node_network_receive_bytes:rate:sum
        expr: sum(rate(node_network_receive_bytes_total[3m])) BY (nodename)
      - record: instance:node_network_transmit_bytes:rate:sum
        expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (nodename)
      - record: cluster:node_cpu:sum_rate5m
        expr: sum(rate(node_cpu_seconds_total{mode!="idle"}[5m]))
      - alert: NodeExporterDown
        expr: absent(up{job="node-exporter-local"} == 1)
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "node:{{ $labels.nodename }}Prometheus could not scrape a node-exporter for more than 10m,or node-exporters have disappeared from discovery"  
      - alert: AlertmanagerReloadFailed
        expr: alertmanager_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}."
          summary: "Alertmanager configuration reload has failed"          
      - alert: NodeMemoryUsage
        expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 > 80
        for: 2m
        labels:
          severity: page
        annotations:
          summary: "{{$labels.instance}}: High Memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 80% (current value is: {{ $value }}"
      - alert: NodeFilesystemUsage
        expr: (node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) / node_filesystem_size_bytes{mountpoint="/"} * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "{{$labels.instance}}: High Filesystem usage detected"
          description: "{{$labels.instance}}: Filesystem usage is above 80% (current value is: {{ $value }}"
      - alert: TargetDown
        expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 4
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "{{ $value }}% of {{ $labels.job }} targets are down."
          summary: "Targets are down"
      - alert: systemdDown
        expr: node_systemd_unit_state{state="active"} < 1
        for: 2m
        labels:
          severity: warning
        annotations:
          description: "{{ $value }}% of {{ $labels.job }} service are down."
          summary: "services are down"
      - alert: diskFullAlertInFutrue
        expr: predict_linear(node_filesystem_free_bytes{job="node-exporter"}[1h], 168*3600)  < 0
        for: 15m
        labels:
          severity: warning
        annotations:
          description: "{{ $labels.name }}% of disk will full in 168hours"
          summary: "services are down"
      - alert: NodeCPUUsage
        expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "{{$labels.instance}}: High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 90% (current value is: {{ $value }}"
      - alert: NodeLoad
        expr: node_load1 > 4
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "{{$labels.nodename}} load1 is above 4 "
          description: "{{$labels.nodename}} load1 is above 4"            
      #kube-state-metrics.rules    
      - alert: DeploymentGenerationMismatch
        expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
        for: 15m
        labels:
          severity: warning
        annotations:
          description: "Observed deployment generation does not match expected one for deployment {{$labels.namespaces}}{{$labels.deployment}}"
      - alert: DeploymentReplicasNotUpdated
        expr: ((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas) or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas)) unless (kube_deployment_spec_paused == 1)
        for: 15m
        labels:
          severity: warning
        annotations:
          description: "Replicas are not updated and available for deployment {{$labels.namespaces}}/{{$labels.deployment}}"
      - alert: DaemonSetRolloutStuck
        expr: kube_daemonset_status_current_number_ready / kube_daemonset_status_desired_number_scheduled
          * 100 < 100
        for: 15m
        labels:
          severity: warning
        annotations:
          description: "Only {{$value}}% of desired pods scheduled and ready for daemon set {{$labels.namespaces}}/{{$labels.daemonset}}"
      - alert: K8SDaemonSetsNotScheduled
        expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "A number of daemonsets are not scheduled."
          summary: "Daemonsets are not scheduled correctly"
      - alert: DaemonSetsMissScheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "A number of daemonsets are running where they are not supposed to run."
          summary: "Daemonsets are not scheduled correctly"
      - alert: PodFrequentlyRestarting
        expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Pod {{$labels.namespaces}}/{{$labels.pod}} is was restarted {{$value}} times within the last hour"
      #kubelet.rules
      - alert: K8SNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 1h
        labels:
          severity: warning
        annotations:
          description: "The Kubelet on {{ $labels.node }} has not checked in with the API, or has set itself to NotReady, for more than an hour"
          summary: "Node status is NotReady"
      - alert: K8SManyNodesNotReady
        expr: count(kube_node_status_condition{condition="Ready",status="true"} == 0) > 1 and (count(kube_node_status_condition{condition="Ready",status="true"} ==0) / count(kube_node_status_condition{condition="Ready",status="true"})) > 0.2
        for: 1m
        labels:
          severity: critical
        annotations:
          description: "{{ $value }}% of Kubernetes nodes are not ready"
      - alert: K8SKubeletTooManyPods
        expr: kubelet_running_pod_count > 100
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Kubelet {{$labels.instance}} is running {{$value}} pods, close to the limit of 110"
          summary: "Kubelet is close to pod limit"
      #kubernetes.rules  
      - alert: APIServerErrorsHigh
        expr: rate(apiserver_request_count{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_count[5m]) * 100 > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "API server returns errors for {{ $value }}% of requests"
      - record: pod_name:container_memory_usage_bytes:sum
        expr: sum(container_memory_usage_bytes{container_name!="POD",pod_name!=""}) BY (pod_name)
      - record: pod_name:container_spec_cpu_shares:sum
        expr: sum(container_spec_cpu_shares{container_name!="POD",pod_name!=""}) BY (pod_name)
      - record: pod_name:container_cpu_usage:sum
        expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD",pod_name!=""}[5m])) BY (pod_name)
      - record: pod_name:container_fs_usage_bytes:sum
        expr: sum(container_fs_usage_bytes{container_name!="POD",pod_name!=""}) BY (pod_name)
      - record: namespace:container_memory_usage_bytes:sum
        expr: sum(container_memory_usage_bytes{container_name!=""}) BY (namespace)
      - record: namespace:container_spec_cpu_shares:sum
        expr: sum(container_spec_cpu_shares{container_name!=""}) BY (namespace)
      - record: namespace:container_cpu_usage:sum
        expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD"}[5m])) BY (namespace)
      - record: cluster:memory_usage:ratio
        expr: sum(container_memory_usage_bytes{container_name!="POD",pod_name!=""}) BY (cluster) / sum(machine_memory_bytes) BY (cluster)
      - record: cluster:container_spec_cpu_shares:ratio
        expr: sum(container_spec_cpu_shares{container_name!="POD",pod_name!=""}) / 1000 / sum(machine_cpu_cores)
      - record: cluster:container_cpu_usage:ratio
        expr: rate(container_cpu_usage_seconds_total{container_name!="POD",pod_name!=""}[5m]) / sum(machine_cpu_cores)
      - record: apiserver_latency_seconds:quantile
        expr: histogram_quantile(0.99, rate(apiserver_request_latencies_bucket[5m])) / 1e+06
        labels:
          quantile: "0.99"
      #prometheus.rules
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}"
      - alert: PrometheusNotificationQueueRunningFull
        expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{$labels.pod}}"
      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) > 0.01
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}"
      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) > 0.03
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}"
      - alert: PrometheusNotConnectedToAlertmanagers
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected to any Alertmanagers"
      #noah_pod.rules
      - alert: Pod_all_cpu_usage
        expr: (sum by(name)(rate(container_cpu_usage_seconds_total{image!=""}[5m]))*100) > 10
        for: 5m
        labels:
          severity: waring
          service: pods
        annotations:
          description: "容器 {{ $labels.name }} CPU 资源利用率大于 75% , (current value is {{ $value }})"
          summary: "Dev CPU 负载告警"
      - alert: Pod_all_memory_usage
        expr: sort_desc(avg by(name)(irate(container_memory_usage_bytes{name!=""}[5m]))*100) > 1024*10^3*8
        for: 10m
        labels:
          severity: waring
        annotations:
          description: "容器 {{ $labels.name }} Memory 资源利用率大于 8G , (current value is {{ $value }})"
          summary: "Dev Memory 负载告警"
      - alert: Pod_all_network_receive_usage
        expr: sum by (name)(irate(container_network_receive_bytes_total{container_name="POD"}[1m])) > 1024*1024*50
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "容器 {{ $labels.name }} network_receive 资源利用率大于 50M , (current value is {{ $value }})"
          summary: "network_receive 负载告警"


      
